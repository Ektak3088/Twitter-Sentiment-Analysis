{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1 - NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape = (7320, 3)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df_train = pd.read_csv('train.csv',encoding='latin1')\n",
    "print('Training Set Shape = {}'.format(df_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@USAirways  ! THE WORST in customer service. @...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@united call wait times are over 20 minutes an...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@JetBlue what's up with the random delay on fl...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@AmericanAir Good morning!  Wondering why my p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@united UA 746. Pacific Rim and Date Night cut...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               text  Target\n",
       "0   1  @USAirways  ! THE WORST in customer service. @...      -1\n",
       "1   2  @united call wait times are over 20 minutes an...      -1\n",
       "2   3  @JetBlue what's up with the random delay on fl...      -1\n",
       "3   4  @AmericanAir Good morning!  Wondering why my p...       0\n",
       "4   5  @united UA 746. Pacific Rim and Date Night cut...      -1"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Text Data for number of mentions, hastags, question marks, url, emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCounts(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def count_regex(self, pattern, tweet):\n",
    "        #finding all the substring containing the pattern in the tweet\n",
    "        return len(re.findall(pattern, tweet))\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # fit method is used when specific operations need to be done on the train data, but not on the test data\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        #all the alphanumeric character\n",
    "        count_words = X.apply(lambda x: self.count_regex(r'\\w+', x)) \n",
    "        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', x))\n",
    "        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', x))\n",
    "        count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', x))\n",
    "        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?+', x))\n",
    "        count_urls = X.apply(lambda x: self.count_regex(r'https?://[^\\s]+[\\s]?', x))\n",
    "        # We will replace the emoji symbols with a description, which makes using a regex for counting easier\n",
    "        # Moreover, it will result in having more words in the tweet\n",
    "        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))\n",
    "        \n",
    "        data = pd.DataFrame({'count_words': count_words\n",
    "                           , 'count_mentions': count_mentions\n",
    "                           , 'count_hashtags': count_hashtags\n",
    "                           , 'count_capital_words': count_capital_words\n",
    "                           , 'count_excl_quest_marks': count_excl_quest_marks\n",
    "                           , 'count_urls': count_urls\n",
    "                           , 'count_emojis': count_emojis\n",
    "                          })\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "0           18               2               2                    2   \n",
       "1           14               1               0                    0   \n",
       "2           19               1               0                    0   \n",
       "3           17               1               0                    1   \n",
       "4           18               1               0                    1   \n",
       "\n",
       "   count_excl_quest_marks  count_urls  count_emojis  \n",
       "0                       3           0             0  \n",
       "1                       0           0             0  \n",
       "2                       2           0             0  \n",
       "3                       2           0             0  \n",
       "4                       0           0             0  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tc = TextCounts()\n",
    "\n",
    "data_eda = tc.fit_transform(df_train.text)\n",
    "data_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "0           18               2               2                    2   \n",
       "1           14               1               0                    0   \n",
       "2           19               1               0                    0   \n",
       "3           17               1               0                    1   \n",
       "4           18               1               0                    1   \n",
       "\n",
       "   count_excl_quest_marks  count_urls  count_emojis  Target  \n",
       "0                       3           0             0      -1  \n",
       "1                       0           0             0      -1  \n",
       "2                       2           0             0      -1  \n",
       "3                       2           0             0       0  \n",
       "4                       0           0             0      -1  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_eda['Target'] = df_train.Target\n",
    "data_eda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Text data (Train Dataset):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changed text to lower case and Removed numbers, URLs, digits, websites, html tags, single alphabets, Special Characaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the worst in customer service   calling for...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>call wait times are over  minutes and airport...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>whats up with the random delay on flight  any...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>good morning  wondering why my pretsa check w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ua  pacific rim and date night cut out not co...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               text  Target\n",
       "0   1     the worst in customer service   calling for...      -1\n",
       "1   2   call wait times are over  minutes and airport...      -1\n",
       "2   3   whats up with the random delay on flight  any...      -1\n",
       "3   4   good morning  wondering why my pretsa check w...       0\n",
       "4   5   ua  pacific rim and date night cut out not co...      -1"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def  clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    # remove numbers\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    #remove urls\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r'http.?://[^\\s]+[\\s]?', '', elem))\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r'{html}', '', elem))\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r'<.*?>', '', elem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "data_clean = clean_text(df_train, 'text')\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removed Wh words and negative words from Stopword dictionary which might changed the sentiment of the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "List of fresh stopwords in English:\n",
      "{'him', 'll', 'at', 'other', 'there', 'do', 'but', 'o', 'ma', 'if', 'yourselves', 'does', 'her', 'further', 'through', 'few', 'than', 'he', 'she', 'a', \"you're\", 'out', 'because', 'that', 'all', 'again', 'before', 'i', 'your', 'hers', 'an', 'own', 'now', 'is', 'more', 'ours', 'it', 'should', 'once', 'of', 't', 'off', \"should've\", 'and', 'by', 'yours', 'herself', 'the', 'y', 'itself', 'himself', 'during', 'his', 'up', 'in', 'from', 'for', 'being', 'about', 'down', 'them', \"you'll\", 'here', \"it's\", 'am', 'until', 'so', 'just', 'be', \"she's\", 'very', 'then', 'same', 've', 'were', 're', 'you', \"you've\", 'we', 'themselves', 'having', 'my', 'each', 'only', 'their', 'after', 'its', 'been', 'did', 'shan', 'are', 'above', 'ourselves', 'can', 'against', 'any', 'under', 'below', 'will', 'with', 'yourself', 'over', 'aren', 'these', 'myself', 'our', 'most', 'into', 'between', 'to', 'this', 'such', 'on', 'doing', 'those', \"that'll\", \"you'd\", 'has', 'm', 'needn', 'was', 'had', 'too', 'while', 'some', 'theirs', 'have', 'both', 's', 'they', 'or', 'as', 'd', 'nor', 'me', 'won'}\n",
      "136\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "stop_words = set(stopwords.words('english')) - set([\"who\", \"what\", \"when\", \"why\", \"how\", \"which\", \"where\", \"whom\", \"no\", \"not\", \"weren't\", \"aren't\",\"didn't\", \"wasn't\", \"couldn't\", \"hadn't\",\"hasn't\", \"doesn't\", \"shouldn't\", \"isn't\", \"wouldn't\", \"don't\", \"mightn't\", \"won't\", \"haven't\", \"mustn\", \"ain\",\"hasn\", \"weren\", \"mustn't\", \"wasn\", \"didn\", \"hadn\", \"don\", \"haven\", \"shouldn\", \"shan't\", \"isn\", \"wouldn\", \"mightn\", \"couldn\", \"needn't\", \"doesn\" ])\n",
    "print(\"\\nList of fresh stopwords in English:\")\n",
    "print (stop_words)\n",
    "print (len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>worst customer service calling month book flig...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>call wait times minutes airport wait times longer</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>whats random delay flight chance false alarm</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>good morning wondering why pretsa check not bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ua pacific rim date night cut not constantly r...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               text  Target\n",
       "0   1  worst customer service calling month book flig...      -1\n",
       "1   2  call wait times minutes airport wait times longer      -1\n",
       "2   3       whats random delay flight chance false alarm      -1\n",
       "3   4  good morning wondering why pretsa check not bo...       0\n",
       "4   5  ua pacific rim date night cut not constantly r...      -1"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean['text'] = data_clean['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ektam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming using Porter Stemmer\n",
    "#### Used Porter and Snowball Stemmer both to check which technique is better, Porter stemmer is giving more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def stemming(self, input_text):\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.stemming)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 records have no words left after text cleaning\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ct = CleanText()\n",
    "sr_clean = ct.fit_transform(data_clean.text)\n",
    "sr_clean.sample(5)\n",
    "empty_clean = sr_clean == ''\n",
    "print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))\n",
    "sr_clean.loc[empty_clean] = '[no_text]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sr_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    worst custom servic call month book flight poo...\n",
       "1        call wait time minut airport wait time longer\n",
       "2            what random delay flight chanc fals alarm\n",
       "3    good morn wonder whi pretsa check not board pa...\n",
       "4    ua pacif rim date night cut not constantli ran...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [bad, custom, servic, call, month, book, fligh...\n",
       "1       [call, wait, time, minut, airport, wait, time,...\n",
       "2        [what, random, delay, flight, chanc, fal, alarm]\n",
       "3       [good, morn, wonder, whi, pretsa, check, not, ...\n",
       "4       [ua, pacif, rim, date, night, cut, not, consta...\n",
       "                              ...                        \n",
       "7315                                         [followback]\n",
       "7316      [thank, help, wish, phone, rep, could, accomid]\n",
       "7317                     [bad, ever, dca, customerservic]\n",
       "7318                      [look, anoth, apolog, not, fli]\n",
       "7319    [far, bad, airlin, plane, delay, round, trip, ...\n",
       "Name: text, Length: 7320, dtype: object"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_clean.apply(lambda x: [y.lemma_ for y in  nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records have no words left after text cleaning\n",
      "0    worst custom servic call month book flight poo...\n",
      "1        call wait time minut airport wait time longer\n",
      "2            what random delay flight chanc fals alarm\n",
      "3    good morn wonder whi pretsa check not board pa...\n",
      "4    ua pacif rim date night cut not constantli ran...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "empty_clean = sr_clean == ''\n",
    "\n",
    "\n",
    "print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))\n",
    "sr_clean.loc[empty_clean] = '[no_text]'\n",
    "print (sr_clean[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aadvantag',\n",
       " 'aafail',\n",
       " 'aaso',\n",
       " 'ab',\n",
       " 'abandon',\n",
       " 'abassinet',\n",
       " 'abc',\n",
       " 'abcnew',\n",
       " 'abi',\n",
       " 'abil',\n",
       " 'abl',\n",
       " 'aboard',\n",
       " 'aboout',\n",
       " 'abound',\n",
       " 'abq',\n",
       " 'absolut',\n",
       " 'absoulut',\n",
       " 'absurd',\n",
       " 'absurdli']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(sr_clean)\n",
    "#printing only first 20\n",
    "cv.get_feature_names()[0:20]\n",
    "#bow.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word  freq\n",
      "0   flight  2378\n",
      "1    thank   855\n",
      "2      get   820\n",
      "3      not   787\n",
      "4       no   749\n",
      "5     hour   587\n",
      "6   cancel   549\n",
      "7     help   528\n",
      "8   servic   494\n",
      "9    delay   487\n",
      "10    time   479\n",
      "11  custom   469\n",
      "12    call   406\n",
      "13    what   402\n",
      "14     bag   397\n",
      "15    wait   381\n",
      "16      im   373\n",
      "17     fli   366\n",
      "18   plane   354\n",
      "19      us   352\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_freq = dict(zip(cv.get_feature_names(), np.asarray(bow.sum(axis=0)).ravel()))\n",
    "word_counter = collections.Counter(word_freq)\n",
    "word_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\n",
    "print(word_counter_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.barplot(x=\"word\", y=\"freq\", data=word_counter_df, palette=\"Blues_d\", ax=ax)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count_words',\n",
       " 'count_mentions',\n",
       " 'count_hashtags',\n",
       " 'count_capital_words',\n",
       " 'count_excl_quest_marks',\n",
       " 'count_urls',\n",
       " 'count_emojis',\n",
       " 'Target',\n",
       " 'clean_text']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_model = data_eda\n",
    "data_model['clean_text'] = sr_clean\n",
    "data_model.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>Target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>worst custom servic call month book flight poo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>call wait time minut airport wait time longer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>what random delay flight chanc fals alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>good morn wonder whi pretsa check not board pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>ua pacif rim date night cut not constantli ran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "0           18               2               2                    2   \n",
       "1           14               1               0                    0   \n",
       "2           19               1               0                    0   \n",
       "3           17               1               0                    1   \n",
       "4           18               1               0                    1   \n",
       "\n",
       "   count_excl_quest_marks  count_urls  count_emojis  Target  \\\n",
       "0                       3           0             0      -1   \n",
       "1                       0           0             0      -1   \n",
       "2                       2           0             0      -1   \n",
       "3                       2           0             0       0   \n",
       "4                       0           0             0      -1   \n",
       "\n",
       "                                          clean_text  \n",
       "0  worst custom servic call month book flight poo...  \n",
       "1      call wait time minut airport wait time longer  \n",
       "2          what random delay flight chanc fals alarm  \n",
       "3  good morn wonder whi pretsa check not board pa...  \n",
       "4  ua pacif rim date night cut not constantli ran...  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Shape = (7320, 2)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df_test = pd.read_csv('test.csv',encoding='latin1')\n",
    "print('Test Set Shape = {}'.format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Text Data for number of mentions, hastags, question marks, url, emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCounts(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def count_regex(self, pattern, tweet):\n",
    "        #finding all the substring containing the pattern in the tweet\n",
    "        return len(re.findall(pattern, tweet))\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # fit method is used when specific operations need to be done on the train data, but not on the test data\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        #all the alphanumeric character\n",
    "        count_words = X.apply(lambda x: self.count_regex(r'\\w+', x)) \n",
    "        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', x))\n",
    "        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', x))\n",
    "        count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', x))\n",
    "        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?+', x))\n",
    "        count_urls = X.apply(lambda x: self.count_regex(r'https?://[^\\s]+[\\s]?', x))\n",
    "        # We will replace the emoji symbols with a description, which makes using a regex for counting easier\n",
    "        # Moreover, it will result in having more words in the tweet\n",
    "        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))\n",
    "        \n",
    "        data = pd.DataFrame({'count_words': count_words\n",
    "                           , 'count_mentions': count_mentions\n",
    "                           , 'count_hashtags': count_hashtags\n",
    "                           , 'count_capital_words': count_capital_words\n",
    "                           , 'count_excl_quest_marks': count_excl_quest_marks\n",
    "                           , 'count_urls': count_urls\n",
    "                           , 'count_emojis': count_emojis\n",
    "                          })\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "0           28               1               0                    2   \n",
       "1           19               1               0                    1   \n",
       "2           25               1               0                    1   \n",
       "3           18               1               0                    1   \n",
       "4            9               1               0                    1   \n",
       "\n",
       "   count_excl_quest_marks  count_urls  count_emojis  \n",
       "0                       1           0             0  \n",
       "1                       0           0             0  \n",
       "2                       1           0             0  \n",
       "3                       0           0             0  \n",
       "4                       1           0             0  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tc = TextCounts()\n",
    "\n",
    "test_eda = tc.fit_transform(df_test.text)\n",
    "test_eda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Text Data (Test Dataset):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changed text to lower case, Removed numbers, URLs, digits, websites, html tags, single alphabets, Special Characaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7322</td>\n",
       "      <td>in car gng to dfw pulled over hr ago  very ic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7323</td>\n",
       "      <td>after all the plane didnt land in identical o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7324</td>\n",
       "      <td>cant believe how many paying customers you le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7325</td>\n",
       "      <td>i can legitimately say that i would have rath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7326</td>\n",
       "      <td>still no response from aa great job guys</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text\n",
       "0  7322   in car gng to dfw pulled over hr ago  very ic...\n",
       "1  7323   after all the plane didnt land in identical o...\n",
       "2  7324   cant believe how many paying customers you le...\n",
       "3  7325   i can legitimately say that i would have rath...\n",
       "4  7326           still no response from aa great job guys"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def  clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    # remove numbers\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    #remove urls\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r'http.?://[^\\s]+[\\s]?', '', elem))\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r'{html}', '', elem))\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r'<.*?>', '', elem))                                      \n",
    "    return df\n",
    "\n",
    "test_clean = clean_text(df_test, 'text')\n",
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removed Wh words and negative words from Stopword dictionary which might changed the sentiment of the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "List of fresh stopwords in English:\n",
      "{'him', 'll', 'at', 'other', 'there', 'do', 'but', 'o', 'ma', 'if', 'yourselves', 'does', 'her', 'further', 'through', 'few', 'than', 'he', 'she', 'a', \"you're\", 'out', 'because', 'that', 'all', 'again', 'before', 'i', 'your', 'hers', 'an', 'own', 'now', 'is', 'more', 'ours', 'it', 'should', 'once', 'of', 't', 'off', \"should've\", 'and', 'by', 'yours', 'herself', 'the', 'y', 'itself', 'himself', 'during', 'his', 'up', 'in', 'from', 'for', 'being', 'about', 'down', 'them', \"you'll\", 'here', \"it's\", 'am', 'until', 'so', 'just', 'be', \"she's\", 'very', 'then', 'same', 've', 'were', 're', 'you', \"you've\", 'we', 'themselves', 'having', 'my', 'each', 'only', 'their', 'after', 'its', 'been', 'did', 'shan', 'are', 'above', 'ourselves', 'can', 'against', 'any', 'under', 'below', 'will', 'with', 'yourself', 'over', 'aren', 'these', 'myself', 'our', 'most', 'into', 'between', 'to', 'this', 'such', 'on', 'doing', 'those', \"that'll\", \"you'd\", 'has', 'm', 'needn', 'was', 'had', 'too', 'while', 'some', 'theirs', 'have', 'both', 's', 'they', 'or', 'as', 'd', 'nor', 'me', 'won'}\n",
      "136\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "stop_words = set(stopwords.words('english')) - set([\"who\", \"what\", \"when\", \"why\", \"how\", \"which\", \"where\", \"whom\", \"no\", \"not\", \"weren't\", \"aren't\",\"didn't\", \"wasn't\", \"couldn't\", \"hadn't\",\"hasn't\", \"doesn't\", \"shouldn't\", \"isn't\", \"wouldn't\", \"don't\", \"mightn't\", \"won't\", \"haven't\", \"mustn\", \"ain\",\"hasn\", \"weren\", \"mustn't\", \"wasn\", \"didn\", \"hadn\", \"don\", \"haven\", \"shouldn\", \"shan't\", \"isn\", \"wouldn\", \"mightn\", \"couldn\", \"needn't\", \"doesn\" ])\n",
    "print(\"\\nList of fresh stopwords in English:\")\n",
    "print (stop_words)\n",
    "print (len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7322</td>\n",
       "      <td>car gng dfw pulled hr ago icy roads onhold aa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7323</td>\n",
       "      <td>plane didnt land identical worse conditions gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7324</td>\n",
       "      <td>cant believe how many paying customers left hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7325</td>\n",
       "      <td>legitimately say would rather driven cross cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7326</td>\n",
       "      <td>still no response aa great job guys</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text\n",
       "0  7322  car gng dfw pulled hr ago icy roads onhold aa ...\n",
       "1  7323  plane didnt land identical worse conditions gr...\n",
       "2  7324  cant believe how many paying customers left hi...\n",
       "3  7325  legitimately say would rather driven cross cou...\n",
       "4  7326                still no response aa great job guys"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean['text'] = test_clean['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming using Porter Stemmer\n",
    "#### Used Porter and Snowball Stemmer both to check which technique is better, Porter stemmer is giving more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ektam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def stemming(self, input_text):\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.stemming)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 records have no words left after text cleaning\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ct = CleanText()\n",
    "sr_clean = ct.fit_transform(test_clean.text)\n",
    "sr_clean.sample(5)\n",
    "empty_clean = sr_clean == ''\n",
    "print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))\n",
    "sr_clean.loc[empty_clean] = '[no_text]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    car gng dfw pull hr ago ici road onhold aa sin...\n",
       "1    plane didnt land ident wors condit grk accord ...\n",
       "2    cant believ how mani pay custom left high dri ...\n",
       "3    legitim say would rather driven cross countri ...\n",
       "4                    still no respons aa great job guy\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [car, gng, dfw, pull, hr, ago, ici, road, onho...\n",
       "1       [plane, do, not, land, ident, wor, condit, grk...\n",
       "2       [can, not, believ, how, mani, pay, custom, lea...\n",
       "3       [legitim, say, would, rather, drive, cross, co...\n",
       "4                [still, no, respon, aa, great, job, guy]\n",
       "                              ...                        \n",
       "7315    [travel, two, kid, tomorrow, age, domest, need...\n",
       "7316    [tx, info, do, not, understand, whi, could, no...\n",
       "7317    [understand, whi, flight, day, not, go, twice,...\n",
       "7318                                             [realli]\n",
       "7319    [no, not, make, connect, stellar, employe, vic...\n",
       "Name: text, Length: 7320, dtype: object"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_clean.apply(lambda x: [y.lemma_ for y in  nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records have no words left after text cleaning\n",
      "0    car gng dfw pull hr ago ici road onhold aa sin...\n",
      "1    plane didnt land ident wors condit grk accord ...\n",
      "2    cant believ how mani pay custom left high dri ...\n",
      "3    legitim say would rather driven cross countri ...\n",
      "4                    still no respons aa great job guy\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "empty_clean = sr_clean == ''\n",
    "\n",
    "print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))\n",
    "sr_clean.loc[empty_clean] = '[no_text]'\n",
    "print (sr_clean[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaaand',\n",
       " 'aaadvantag',\n",
       " 'aaalwaysl',\n",
       " 'aaba',\n",
       " 'aacom',\n",
       " 'aadavantag',\n",
       " 'aadelay',\n",
       " 'aadfw',\n",
       " 'aadv',\n",
       " 'aadvantag',\n",
       " 'aafail',\n",
       " 'aal',\n",
       " 'aampc',\n",
       " 'aano',\n",
       " 'aaron',\n",
       " 'aateam',\n",
       " 'aau',\n",
       " 'aback',\n",
       " 'abandon']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(sr_clean)\n",
    "#printing only first 20\n",
    "cv.get_feature_names()[0:20]\n",
    "#bow.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word  freq\n",
      "0   flight  2428\n",
      "1    thank   833\n",
      "2      get   797\n",
      "3      not   777\n",
      "4       no   722\n",
      "5     hour   563\n",
      "6     help   514\n",
      "7   cancel   507\n",
      "8   servic   495\n",
      "9    delay   493\n",
      "10    time   480\n",
      "11  custom   467\n",
      "12    what   417\n",
      "13      im   391\n",
      "14   plane   366\n",
      "15    wait   364\n",
      "16    call   362\n",
      "17     bag   359\n",
      "18      us   353\n",
      "19    hold   351\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_freq = dict(zip(cv.get_feature_names(), np.asarray(bow.sum(axis=0)).ravel()))\n",
    "word_counter = collections.Counter(word_freq)\n",
    "word_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\n",
    "print(word_counter_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.barplot(x=\"word\", y=\"freq\", data=word_counter_df, palette=\"Blues_d\", ax=ax)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count_words',\n",
       " 'count_mentions',\n",
       " 'count_hashtags',\n",
       " 'count_capital_words',\n",
       " 'count_excl_quest_marks',\n",
       " 'count_urls',\n",
       " 'count_emojis',\n",
       " 'clean_text']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = test_eda\n",
    "data_test['clean_text'] = sr_clean\n",
    "data_test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>car gng dfw pull hr ago ici road onhold aa sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>plane didnt land ident wors condit grk accord ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cant believ how mani pay custom left high dri ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>legitim say would rather driven cross countri ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>still no respons aa great job guy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7315</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>travel two kid tomorrow age domest need birth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7316</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tx info dont understand whi couldnt accur esti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7317</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>understand whi flight day not go twice im extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7318</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>realli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7319</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no not make connect stellar employe vicki thom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7320 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "0              28               1               0                    2   \n",
       "1              19               1               0                    1   \n",
       "2              25               1               0                    1   \n",
       "3              18               1               0                    1   \n",
       "4               9               1               0                    1   \n",
       "...           ...             ...             ...                  ...   \n",
       "7315           21               1               0                    0   \n",
       "7316           24               1               0                    0   \n",
       "7317           28               1               0                    0   \n",
       "7318            2               1               0                    0   \n",
       "7319           23               1               0                    0   \n",
       "\n",
       "      count_excl_quest_marks  count_urls  count_emojis  \\\n",
       "0                          1           0             0   \n",
       "1                          0           0             0   \n",
       "2                          1           0             0   \n",
       "3                          0           0             0   \n",
       "4                          1           0             0   \n",
       "...                      ...         ...           ...   \n",
       "7315                       1           0             0   \n",
       "7316                       0           0             0   \n",
       "7317                       3           0             0   \n",
       "7318                       2           0             0   \n",
       "7319                       0           0             0   \n",
       "\n",
       "                                             clean_text  \n",
       "0     car gng dfw pull hr ago ici road onhold aa sin...  \n",
       "1     plane didnt land ident wors condit grk accord ...  \n",
       "2     cant believ how mani pay custom left high dri ...  \n",
       "3     legitim say would rather driven cross countri ...  \n",
       "4                     still no respons aa great job guy  \n",
       "...                                                 ...  \n",
       "7315  travel two kid tomorrow age domest need birth ...  \n",
       "7316  tx info dont understand whi couldnt accur esti...  \n",
       "7317  understand whi flight day not go twice im extr...  \n",
       "7318                                             realli  \n",
       "7319  no not make connect stellar employe vicki thom...  \n",
       "\n",
       "[7320 rows x 8 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        #print(\"I am in\")\n",
    "        \n",
    "        #print(X[self.cols].dtype())\n",
    "        return X[self.cols]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data (70:30)\n",
    "#### Note: I checked the accuracy by splitting data 75:25 and 70:30 (Train and Test). 70:30 division is providing better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_model.drop('Target', axis=1), data_model.Target, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defined Scoring method as F1 Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_vect(clf, parameters_clf, X_train, X_test, parameters_text=None, vect=None, is_w2v=False):\n",
    "    \n",
    "    textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'\n",
    "                      ,'count_mentions','count_urls','count_words']\n",
    "    \n",
    "    if is_w2v:\n",
    "        w2vcols = []\n",
    "        print(\"not good\")\n",
    "        for i in range(SIZE):\n",
    "            w2vcols.append(i)\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('w2v', ColumnExtractor(cols=w2vcols))]\n",
    "                                , n_jobs=-1)\n",
    "    else:\n",
    "        print(\"ok\")\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text')), ('vect', vect)]))]\n",
    "                                , n_jobs=1)\n",
    "\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', features)\n",
    "        , ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    parameters = dict()\n",
    "    if parameters_text:\n",
    "        parameters.update(parameters_text)\n",
    "    parameters.update(parameters_clf)\n",
    "    \n",
    "    # Make sure you have scikit-learn version 0.19 or higher to use multiple scoring metrics\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5, scoring='f1_macro')\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best CV score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    print(\"Train score with best_estimator_: %0.3f\" % grid_search.best_estimator_.score(X_train, y_train))\n",
    "    print(\"\\n\")\n",
    "    print(\"Test score with best_estimator_: %0.3f\" % grid_search.best_estimator_.score(X_test, y_test))\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report Test Data\")\n",
    "    print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))\n",
    "    print(\"Classification Report Train Data\")\n",
    "    print(classification_report(y_train, grid_search.best_estimator_.predict(X_train)))                    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for GridSearch (with n-gram bag of words approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Count and TFIDF)\n",
    "parameters_vect = {\n",
    "    'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
    "    'features__pipe__vect__ngram_range': ((1,1), (1,2)),\n",
    "    'features__pipe__vect__min_df': (1,2)\n",
    "}\n",
    "\n",
    "\n",
    "# Parameter grid settings for MultinomialNB\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': (0.10,0.25, 0.5, 0.75,1.0,1.25)\n",
    "}\n",
    "\n",
    "\n",
    "# Parameter grid settings for LogisticRegression\n",
    "parameters_logreg = {\n",
    "    'clf__C': (1.05, 1.10,1.15,1.20,1.25),\n",
    "    'clf__penalty': ('l1', 'l2'),\n",
    "    'clf__multi_class':['auto'],\n",
    "    'clf__solver':['lbfgs', 'liblinear'],\n",
    "    'clf__max_iter': [500]\n",
    "}\n",
    "\n",
    "# Parameter grid settings for XGBoost \n",
    "parameters_xgboost = {\n",
    "    'clf__min_child_weight': (1,5),\n",
    "    'clf__gamma': (0.5,1,1.5,2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "logreg = LogisticRegression()\n",
    "xgboost = XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using three Vectorization techniques Count, Tfidf and Hash with Machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nave Bayes with Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.1, 0.25, 0.5, 0.75, 1.0, 1.25),\n",
      " 'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
      " 'features__pipe__vect__min_df': (1, 2),\n",
      " 'features__pipe__vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   20.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 20.507s\n",
      "\n",
      "Best CV score: 0.690\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.25\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 2\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 2)\n",
      "Train score with best_estimator_: 0.906\n",
      "\n",
      "\n",
      "Test score with best_estimator_: 0.772\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.85      0.88      0.87      1414\n",
      "           0       0.55      0.46      0.50       437\n",
      "           1       0.68      0.71      0.70       345\n",
      "\n",
      "    accuracy                           0.77      2196\n",
      "   macro avg       0.69      0.69      0.69      2196\n",
      "weighted avg       0.76      0.77      0.77      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.94      0.93      0.94      3152\n",
      "           0       0.83      0.84      0.84      1099\n",
      "           1       0.87      0.90      0.89       873\n",
      "\n",
      "    accuracy                           0.91      5124\n",
      "   macro avg       0.88      0.89      0.89      5124\n",
      "weighted avg       0.91      0.91      0.91      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb_countvect = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_countvect_predict = mnb_countvect.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7315</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7316</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7317</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7318</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7319</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7320 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target\n",
       "0         -1\n",
       "1         -1\n",
       "2         -1\n",
       "3          0\n",
       "4         -1\n",
       "...      ...\n",
       "7315       0\n",
       "7316      -1\n",
       "7317      -1\n",
       "7318       1\n",
       "7319      -1\n",
       "\n",
       "[7320 rows x 1 columns]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_countvect_predict1 = pd.DataFrame(mnb_countvect_predict,index=df_test.index,columns=['Target'])\n",
    "mnb_countvect_predict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic with Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (1.05, 1.1, 1.15, 1.2, 1.25),\n",
      " 'clf__max_iter': [500],\n",
      " 'clf__multi_class': ['auto'],\n",
      " 'clf__penalty': ('l1', 'l2'),\n",
      " 'clf__solver': ['lbfgs', 'liblinear'],\n",
      " 'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
      " 'features__pipe__vect__min_df': (1, 2),\n",
      " 'features__pipe__vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed: 10.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 649.291s\n",
      "\n",
      "Best CV score: 0.714\n",
      "Best parameters set:\n",
      "\tclf__C: 1.1\n",
      "\tclf__max_iter: 500\n",
      "\tclf__multi_class: 'auto'\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 1\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 1)\n",
      "Train score with best_estimator_: 0.930\n",
      "\n",
      "\n",
      "Test score with best_estimator_: 0.796\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.85      0.91      0.88      1414\n",
      "           0       0.62      0.52      0.57       437\n",
      "           1       0.75      0.67      0.71       345\n",
      "\n",
      "    accuracy                           0.80      2196\n",
      "   macro avg       0.74      0.70      0.72      2196\n",
      "weighted avg       0.79      0.80      0.79      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.94      0.98      0.96      3152\n",
      "           0       0.90      0.84      0.87      1099\n",
      "           1       0.93      0.87      0.90       873\n",
      "\n",
      "    accuracy                           0.93      5124\n",
      "   macro avg       0.92      0.90      0.91      5124\n",
      "weighted avg       0.93      0.93      0.93      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_countvect = grid_vect(logreg, parameters_logreg, X_train, X_test, parameters_text=parameters_vect, vect=countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_countvect_predict = logreg_countvect.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7315</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7316</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7317</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7318</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7319</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7320 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target\n",
       "0         -1\n",
       "1         -1\n",
       "2         -1\n",
       "3          0\n",
       "4         -1\n",
       "...      ...\n",
       "7315       0\n",
       "7316      -1\n",
       "7317      -1\n",
       "7318       0\n",
       "7319      -1\n",
       "\n",
       "[7320 rows x 1 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_countvect_predict1 = pd.DataFrame(logreg_countvect_predict,index=df_test.index,columns=['Target'])\n",
    "logreg_countvect_predict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost with Count Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__gamma': (0.5, 1, 1.5, 2),\n",
      " 'clf__min_child_weight': (1, 5),\n",
      " 'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
      " 'features__pipe__vect__min_df': (1, 2),\n",
      " 'features__pipe__vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   46.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 503.354s\n",
      "\n",
      "Best CV score: 0.703\n",
      "Best parameters set:\n",
      "\tclf__gamma: 0.5\n",
      "\tclf__min_child_weight: 1\n",
      "\tfeatures__pipe__vect__max_df: 0.5\n",
      "\tfeatures__pipe__vect__min_df: 1\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 1)\n",
      "Train score with best_estimator_: 0.890\n",
      "\n",
      "\n",
      "Test score with best_estimator_: 0.782\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.91      0.87      1414\n",
      "           0       0.61      0.51      0.56       437\n",
      "           1       0.75      0.60      0.67       345\n",
      "\n",
      "    accuracy                           0.78      2196\n",
      "   macro avg       0.73      0.67      0.70      2196\n",
      "weighted avg       0.77      0.78      0.77      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.90      0.96      0.93      3152\n",
      "           0       0.83      0.77      0.79      1099\n",
      "           1       0.92      0.79      0.85       873\n",
      "\n",
      "    accuracy                           0.89      5124\n",
      "   macro avg       0.88      0.84      0.86      5124\n",
      "weighted avg       0.89      0.89      0.89      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgboost_countvect = grid_vect(xgboost, parameters_xgboost, X_train, X_test, parameters_text=parameters_vect, vect=countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_countvect_predict = xgboost_countvect.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7315</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7316</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7317</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7318</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7319</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7320 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target\n",
       "0         -1\n",
       "1         -1\n",
       "2         -1\n",
       "3         -1\n",
       "4          1\n",
       "...      ...\n",
       "7315       0\n",
       "7316      -1\n",
       "7317      -1\n",
       "7318       1\n",
       "7319      -1\n",
       "\n",
       "[7320 rows x 1 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_countvect_predict1 = pd.DataFrame(xgboost_countvect_predict,index=df_test.index,columns=['Target'])\n",
    "xgboost_countvect_predict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Tfidf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nave Bayes with Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.1, 0.25, 0.5, 0.75, 1.0, 1.25),\n",
      " 'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
      " 'features__pipe__vect__min_df': (1, 2),\n",
      " 'features__pipe__vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   16.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 16.243s\n",
      "\n",
      "Best CV score: 0.635\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 2\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 2)\n",
      "Train score with best_estimator_: 0.900\n",
      "\n",
      "\n",
      "Test score with best_estimator_: 0.770\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.95      0.86      1414\n",
      "           0       0.64      0.36      0.46       437\n",
      "           1       0.73      0.57      0.64       345\n",
      "\n",
      "    accuracy                           0.77      2196\n",
      "   macro avg       0.72      0.63      0.66      2196\n",
      "weighted avg       0.75      0.77      0.75      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.89      0.97      0.93      3152\n",
      "           0       0.90      0.75      0.82      1099\n",
      "           1       0.92      0.82      0.87       873\n",
      "\n",
      "    accuracy                           0.90      5124\n",
      "   macro avg       0.91      0.85      0.87      5124\n",
      "weighted avg       0.90      0.90      0.90      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multinomialnb_tfidf = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialnb_tfidf_predict = multinomialnb_tfidf.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7315</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7316</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7317</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7318</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7319</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7320 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target\n",
       "0         -1\n",
       "1         -1\n",
       "2         -1\n",
       "3         -1\n",
       "4         -1\n",
       "...      ...\n",
       "7315      -1\n",
       "7316      -1\n",
       "7317      -1\n",
       "7318       1\n",
       "7319      -1\n",
       "\n",
       "[7320 rows x 1 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomialnb_tfidf_predict1 = pd.DataFrame(multinomialnb_tfidf_predict,index=df_test.index,columns=['Target'])\n",
    "multinomialnb_tfidf_predict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic with Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (1.05, 1.1, 1.15, 1.2, 1.25),\n",
      " 'clf__max_iter': [500],\n",
      " 'clf__multi_class': ['auto'],\n",
      " 'clf__penalty': ('l1', 'l2'),\n",
      " 'clf__solver': ['lbfgs'],\n",
      " 'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
      " 'features__pipe__vect__min_df': (1, 2),\n",
      " 'features__pipe__vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  9.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 588.604s\n",
      "\n",
      "Best CV score: 0.697\n",
      "Best parameters set:\n",
      "\tclf__C: 1.1\n",
      "\tclf__max_iter: 500\n",
      "\tclf__multi_class: 'auto'\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'lbfgs'\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 1\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 1)\n",
      "Train score with best_estimator_: 0.878\n",
      "\n",
      "\n",
      "Test score with best_estimator_: 0.791\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.94      0.88      1414\n",
      "           0       0.66      0.46      0.55       437\n",
      "           1       0.78      0.58      0.66       345\n",
      "\n",
      "    accuracy                           0.79      2196\n",
      "   macro avg       0.75      0.66      0.70      2196\n",
      "weighted avg       0.78      0.79      0.78      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.87      0.98      0.92      3152\n",
      "           0       0.87      0.70      0.77      1099\n",
      "           1       0.94      0.73      0.82       873\n",
      "\n",
      "    accuracy                           0.88      5124\n",
      "   macro avg       0.89      0.80      0.84      5124\n",
      "weighted avg       0.88      0.88      0.87      5124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ektam\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "logisticreg_tfidf = grid_vect(logreg, parameters_logreg, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticreg_tfidf_predict = logisticreg_tfidf.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7315</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7316</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7317</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7318</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7319</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7320 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target\n",
       "0         -1\n",
       "1         -1\n",
       "2         -1\n",
       "3          0\n",
       "4         -1\n",
       "...      ...\n",
       "7315      -1\n",
       "7316      -1\n",
       "7317      -1\n",
       "7318       1\n",
       "7319      -1\n",
       "\n",
       "[7320 rows x 1 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticreg_tfidf_predict1 = pd.DataFrame(logisticreg_tfidf_predict,index=df_test.index,columns=['Target'])\n",
    "logisticreg_tfidf_predict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost with Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__gamma': (0.5, 1, 1.5, 2),\n",
      " 'clf__min_child_weight': (1, 5),\n",
      " 'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
      " 'features__pipe__vect__min_df': (1, 2),\n",
      " 'features__pipe__vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed: 13.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 794.370s\n",
      "\n",
      "Best CV score: 0.705\n",
      "Best parameters set:\n",
      "\tclf__gamma: 0.5\n",
      "\tclf__min_child_weight: 1\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 1\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 2)\n",
      "Train score with best_estimator_: 0.912\n",
      "\n",
      "\n",
      "Test score with best_estimator_: 0.778\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.89      0.87      1414\n",
      "           0       0.60      0.53      0.57       437\n",
      "           1       0.69      0.62      0.65       345\n",
      "\n",
      "    accuracy                           0.78      2196\n",
      "   macro avg       0.71      0.68      0.69      2196\n",
      "weighted avg       0.77      0.78      0.77      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.96      0.94      3152\n",
      "           0       0.85      0.83      0.84      1099\n",
      "           1       0.94      0.83      0.88       873\n",
      "\n",
      "    accuracy                           0.91      5124\n",
      "   macro avg       0.90      0.87      0.89      5124\n",
      "weighted avg       0.91      0.91      0.91      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgboost_tfidf = grid_vect(xgboost, parameters_xgboost, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_tfidf_predict = xgboost_tfidf.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7315</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7316</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7317</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7318</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7319</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7320 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target\n",
       "0         -1\n",
       "1         -1\n",
       "2         -1\n",
       "3         -1\n",
       "4          1\n",
       "...      ...\n",
       "7315       0\n",
       "7316      -1\n",
       "7317      -1\n",
       "7318       0\n",
       "7319      -1\n",
       "\n",
       "[7320 rows x 1 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_tfidf_predict1 = pd.DataFrame(xgboost_tfidf_predict,index=df_test.index,columns=['Target'])\n",
    "xgboost_tfidf_predict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Hash Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "Hashvect = HashingVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Hash)\n",
    "parameters_vect1 = {\n",
    "    'features__pipe__vect__ngram_range': ((1,1), (1,2))\n",
    "}\n",
    "\n",
    "# Parameter grid settings for LogisticRegression\n",
    "parameters_logreg = {\n",
    "    'clf__C': (1.05, 1.10),\n",
    "    'clf__penalty': ('l1', 'l2'),\n",
    "    'clf__multi_class':['auto'],\n",
    "    'clf__solver':['lbfgs'],\n",
    "    'clf__max_iter': [50]\n",
    "}\n",
    "\n",
    "# Parameter grid settings for XGBoost \n",
    "parameters_xgboost = {\n",
    "    'clf__min_child_weight': (1,2),\n",
    "    'clf__gamma': (0.5,1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic with HashVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (1.05, 1.1),\n",
      " 'clf__max_iter': [50],\n",
      " 'clf__multi_class': ['auto'],\n",
      " 'clf__penalty': ('l1', 'l2'),\n",
      " 'clf__solver': ['lbfgs'],\n",
      " 'features__pipe__vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  8.4min finished\n",
      "C:\\Users\\ektam\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 545.328s\n",
      "\n",
      "Best CV score: 0.558\n",
      "Best parameters set:\n",
      "\tclf__C: 1.05\n",
      "\tclf__max_iter: 50\n",
      "\tclf__multi_class: 'auto'\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'lbfgs'\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 1)\n",
      "Train score with best_estimator_: 0.713\n",
      "\n",
      "\n",
      "Test score with best_estimator_: 0.722\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.93      0.83      1414\n",
      "           0       0.52      0.30      0.38       437\n",
      "           1       0.69      0.42      0.52       345\n",
      "\n",
      "    accuracy                           0.72      2196\n",
      "   macro avg       0.65      0.55      0.58      2196\n",
      "weighted avg       0.70      0.72      0.69      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.93      0.82      3152\n",
      "           0       0.55      0.30      0.39      1099\n",
      "           1       0.73      0.45      0.56       873\n",
      "\n",
      "    accuracy                           0.71      5124\n",
      "   macro avg       0.67      0.56      0.59      5124\n",
      "weighted avg       0.69      0.71      0.68      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_Hashvect = grid_vect(logreg, parameters_logreg, X_train, X_test, parameters_text=parameters_vect1, vect=Hashvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_Hashvect_predict = logreg_Hashvect.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7315</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7316</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7317</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7318</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7319</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7320 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target\n",
       "0         -1\n",
       "1         -1\n",
       "2         -1\n",
       "3         -1\n",
       "4         -1\n",
       "...      ...\n",
       "7315      -1\n",
       "7316      -1\n",
       "7317      -1\n",
       "7318       1\n",
       "7319      -1\n",
       "\n",
       "[7320 rows x 1 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_Hashvect_predict1 = pd.DataFrame(logreg_Hashvect_predict,index=df_test.index,columns=['Target'])\n",
    "logreg_Hashvect_predict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost with HashVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__gamma': (0.5, 1),\n",
      " 'clf__min_child_weight': (1, 2),\n",
      " 'features__pipe__vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed: 52.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3300.706s\n",
      "\n",
      "Best CV score: 0.696\n",
      "Best parameters set:\n",
      "\tclf__gamma: 0.5\n",
      "\tclf__min_child_weight: 2\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 2)\n",
      "Train score with best_estimator_: 0.897\n",
      "\n",
      "\n",
      "Test score with best_estimator_: 0.784\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.91      0.87      1414\n",
      "           0       0.61      0.51      0.55       437\n",
      "           1       0.74      0.61      0.67       345\n",
      "\n",
      "    accuracy                           0.78      2196\n",
      "   macro avg       0.73      0.68      0.70      2196\n",
      "weighted avg       0.77      0.78      0.78      2196\n",
      "\n",
      "Classification Report Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.91      0.96      0.93      3152\n",
      "           0       0.84      0.77      0.80      1099\n",
      "           1       0.92      0.82      0.87       873\n",
      "\n",
      "    accuracy                           0.90      5124\n",
      "   macro avg       0.89      0.85      0.87      5124\n",
      "weighted avg       0.90      0.90      0.89      5124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgboost_Hashvect = grid_vect(xgboost, parameters_xgboost, X_train, X_test, parameters_text=parameters_vect1, vect=Hashvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_Hashvect_predict = xgboost_Hashvect.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7315</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7316</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7317</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7318</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7319</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7320 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Target\n",
       "0         -1\n",
       "1         -1\n",
       "2         -1\n",
       "3         -1\n",
       "4          1\n",
       "...      ...\n",
       "7315       0\n",
       "7316      -1\n",
       "7317      -1\n",
       "7318       0\n",
       "7319      -1\n",
       "\n",
       "[7320 rows x 1 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_Hashvect_predict1 = pd.DataFrame(xgboost_Hashvect_predict,index=df_test.index,columns=['Target'])\n",
    "xgboost_Hashvect_predict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Count Vectorization gives highest accuracy score (F1 Score). Therefore, I used the same model to predict target values for test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done :D\n"
     ]
    }
   ],
   "source": [
    "# Create predictions to be submitted!\n",
    "pd.DataFrame({'Id': df_test.id, 'Target': logreg_countvect_predict}).to_csv('solution_base.csv', index =False)  \n",
    "print(\"Done :D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
